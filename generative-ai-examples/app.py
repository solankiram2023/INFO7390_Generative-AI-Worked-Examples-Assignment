import streamlit as st
import numpy as np
import matplotlib.pyplot as plt
import nltk
import pandas as pd
from wordcloud import WordCloud
import time
from PIL import Image
import io
import base64

# Download NLTK data (if not already downloaded)
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

# Set page title and configuration
st.set_page_config(
    page_title="Generative AI Text Models Comparison",
    page_icon="üìù",
    layout="wide",
    initial_sidebar_state="expanded",
)

# Custom CSS for better styling
st.markdown("""
<style>
    .main {
        background-color: #f8f9fa;
    }
    .stTextInput > div > div > input {
        background-color: #ffffff;
    }
    .stTextArea > div > div > textarea {
        background-color: #ffffff;
    }
    h1, h2, h3 {
        color: #1e3a8a;
    }
    .model-output {
        background-color: #ffffff;
        border-radius: 5px;
        padding: 20px;
        border-left: 5px solid #4338ca;
        margin: 10px 0;
    }
    .metrics-card {
        background-color: #ffffff;
        border-radius: 5px;
        padding: 15px;
        margin: 10px 0;
        box-shadow: 0 1px 3px rgba(0,0,0,0.1);
    }
    .stProgress > div > div > div > div {
        background-color: #4338ca;
    }
</style>
""", unsafe_allow_html=True)

# Helper function to analyze text
def analyze_text(text):
    # Tokenize text into sentences and words
    sentences = nltk.sent_tokenize(text)
    words = nltk.word_tokenize(text.lower())
    
    # Remove punctuation from words
    words = [word for word in words if word.isalnum()]
    
    # Calculate metrics
    word_count = len(words)
    avg_word_length = sum(len(word) for word in words) / max(1, word_count)
    unique_words = len(set(words))
    lexical_diversity = unique_words / max(1, word_count)
    avg_sentence_length = word_count / max(1, len(sentences))
    
    return {
        "word_count": word_count,
        "avg_word_length": avg_word_length,
        "unique_words": unique_words,
        "lexical_diversity": lexical_diversity,
        "avg_sentence_length": avg_sentence_length,
        "sentence_count": len(sentences)
    }

# Simulate OpenWebText model output
def simulate_openwebtext_generation(prompt, max_length=100, temperature=0.8):
    # These are simulated responses based on the characteristics of text
    # that would be generated by a model trained on OpenWebText
    responses = {
        "The future of artificial intelligence is":
            "The future of artificial intelligence is likely to be transformative across many industries. AI systems are becoming increasingly capable of performing complex tasks that previously required human intelligence. According to recent research, we can expect significant advancements in natural language processing, computer vision, and reinforcement learning in the next decade.\n\nHowever, there are also important ethical considerations around AI development, including issues of bias, privacy, and job displacement. Many tech companies are now investing in responsible AI frameworks to address these concerns.",

        "Once upon a time in a distant land":
            "Once upon a time in a distant land, a team of researchers discovered an ancient technology buried beneath the desert sands. The device, apparently thousands of years old, contained what appeared to be advanced circuitry unlike anything seen in modern electronics.\n\nThe discovery went viral on social media, with #AncientTech trending worldwide. Scientists from various disciplines formed a collaborative project to study the artifact, while conspiracy theorists claimed it was evidence of alien visitation. The research team set up a live stream of their work, attracting millions of viewers.",

        "The meaning of life is":
            "The meaning of life is a question that has been debated by philosophers, scientists, and religious thinkers throughout human history. Some argue that life has no inherent meaning beyond what we create for ourselves. Others find meaning through connection with others, pursuit of happiness, or contribution to society.\n\nA 2019 survey found that 58% of respondents believed the meaning of life comes from relationships with family and friends, while 25% cited career achievement, and 17% mentioned spiritual fulfillment. Ultimately, many psychologists suggest that finding personal meaning is essential for mental well-being.",

        "To be or not to be":
            "To be or not to be remains one of the most famous lines from Shakespeare's Hamlet, representing the ultimate existential question. In today's context, this phrase has been adopted in popular culture as a shorthand for difficult decisions.\n\nThe concept has been explored in various modern media, including a 2020 Netflix documentary examining how people make life-altering choices. Neuroscience research suggests that this type of existential questioning activates regions in the prefrontal cortex associated with self-reflection and decision-making. A recent study found that 72% of participants reported experiencing major existential questions during significant life transitions.",

        "In the beginning":
            "In the beginning of the internet era, few people predicted how dramatically digital technology would transform society. What started as a network connecting research institutions has evolved into a global system that shapes everything from how we shop to how we find romantic partners.\n\nAccording to data from the International Telecommunications Union, global internet penetration reached 63% in 2021, with mobile devices being the primary access point for most users. Tech analysts predict that the next phase of digital evolution will focus on augmented reality, decentralized web architecture, and AI-driven interfaces that further blur the line between physical and digital experiences.",
    }

    # Add randomness based on temperature
    rand_factor = min(1, max(0, temperature)) * 0.3
    use_cached = np.random.random() > rand_factor

    # Return the matching response or a generic one if no match
    output = responses.get(prompt) if use_cached and prompt in responses else f"{prompt} going to revolutionize how we interact with technology. Recent studies show that advancements in this field are accelerating faster than predicted. According to data from leading research institutions, we can expect significant breakthroughs within the next 5-10 years.\n\nExperts in the field suggest that these developments will impact everything from healthcare to transportation. However, there are also important ethical considerations that need to be addressed, including privacy concerns and potential economic disruption."

    # Truncate to max_length words if needed
    words = output.split()
    if len(words) > max_length:
        return " ".join(words[:max_length]) + "..."
    return output

# Simulate Gutenberg model output
def simulate_gutenberg_generation(prompt, max_length=100, temperature=0.8):
    # These are simulated responses based on the characteristics of text
    # that would be generated by a model trained on Gutenberg
    responses = {
        "The future of artificial intelligence is":
            "The future of artificial intelligence is shrouded in mystery, like a fog upon the distant hills at dawn. Man hath ever sought to create in his own image, and in this modern age, such creation takes the form of thinking machines.\n\nPerchance these mechanical minds shall bring forth a golden age of prosperity, wherein the burdens of labor are lifted from the shoulders of mankind. Yet one must also consider the grave possibility that these creations may, like Prometheus unbound, bring forth consequences unforeseen by their creators. It behooves the wise to proceed with caution in this most momentous of endeavors.",

        "Once upon a time in a distant land":
            "Once upon a time in a distant land, there dwelt a noble king whose wisdom was renowned throughout the realm. His castle stood upon a lofty hill, its spires reaching toward the heavens as if in supplication to the divine.\n\nThe king had three daughters, each fairer than the last, with countenances that would cause the stars themselves to dim with envy. The eldest possessed hair like spun gold and eyes of sapphire blue; the middle daughter's voice was such that nightingales would fall silent in shame; and the youngest, ah! the youngest was blessed with such grace and kindness that flowers would bloom beneath her gentle footsteps.",

        "The meaning of life is":
            "The meaning of life is a question that hath vexed the minds of philosophers since time immemorial. 'Tis a query that echoes through the corridors of human thought, from Socrates to the humble peasant tilling the soil.\n\nSome men find purpose in the accumulation of worldly treasures, though such riches oft prove fleeting as morning dew. Others seek meaning in the pursuit of knowledge, filling their minds with wisdom as one might fill a vessel with precious oil. Yet others find their purpose in love‚Äîthat most exalted of emotions which, as the poet says, 'bears it out even to the edge of doom.'",

        "To be or not to be":
            "To be or not to be‚Äîthat is the question which hath troubled mankind since the dawn of consciousness. Whether 'tis nobler in the mind to suffer the slings and arrows of outrageous fortune, or to take arms against a sea of troubles, and by opposing end them?\n\nThe soul, when faced with mortal uncertainty, must weigh the known trials of existence against the unknown country of death, from whose bourn no traveler returns. Thus conscience doth make cowards of us all; and thus the native hue of resolution is sicklied o'er with the pale cast of thought.",

        "In the beginning":
            "In the beginning, when the world was yet young and the firmament newly stretched above the earth, there lived a race of beings whose nature was half mortal and half divine. They walked upon the verdant hills and through the shadowed valleys, their footsteps leaving marks that would remain for countless ages.\n\nThe sun, in those distant days, shone with a golden light more pure than any known to modern man, and the moon cast silver beams that could heal the sick and comfort the weary of heart. The very air was filled with music‚Äînot the crude melodies of human invention, but the celestial harmonies of the spheres themselves.",
    }
    
    # Add randomness based on temperature
    rand_factor = min(1, max(0, temperature)) * 0.3
    use_cached = np.random.random() > rand_factor

    # Return the matching response or a generic one if no match
    output = responses.get(prompt) if use_cached and prompt in responses else f"{prompt} hath ever been a subject of great contemplation among learned men. As the esteemed philosopher once proclaimed in his treatise on natural philosophy, the nature of such matters lies beyond the common understanding.\n\nIt calls to mind the ancient tale of the wise king who, upon his deathbed, gathered his three sons to impart his final wisdom. \"My children,\" said he, \"seek not the transient pleasures of earthly existence, but rather the eternal truths that lie hidden beneath the veil of ordinary perception.\"\n\nThus it has always been, and thus shall it remain until the final trumpet sounds."

    # Truncate to max_length words if needed
    words = output.split()
    if len(words) > max_length:
        return " ".join(words[:max_length]) + "..."
    return output

# Generate a wordcloud from text
def generate_wordcloud(text):
    wordcloud = WordCloud(
        width=800, 
        height=400, 
        background_color='white',
        max_words=100,
        contour_width=1
    ).generate(text)
    
    # Convert the wordcloud to an image
    img = wordcloud.to_image()
    
    # Convert to bytes for Streamlit
    buf = io.BytesIO()
    img.save(buf, format='PNG')
    byte_im = buf.getvalue()
    
    return byte_im

# Plot metrics comparison
def plot_metrics_comparison(metrics1, metrics2, title="Text Characteristics Comparison"):
    # Select numeric metrics to compare
    metrics_to_plot = ['word_count', 'avg_word_length', 'unique_words', 'lexical_diversity', 'avg_sentence_length']
    labels = ['Word Count', 'Avg Word Length', 'Unique Words', 'Lexical Diversity', 'Avg Sentence Length']
    
    # Create a DataFrame for plotting
    data = {
        'Metric': labels,
        'OpenWebText': [metrics1[m] for m in metrics_to_plot],
        'Gutenberg': [metrics2[m] for m in metrics_to_plot]
    }
    df = pd.DataFrame(data)
    
    # Create the plot
    fig, ax = plt.subplots(figsize=(10, 5))
    
    # Normalize values for better visualization
    max_values = df[['OpenWebText', 'Gutenberg']].max()
    df_normalized = df.copy()
    df_normalized['OpenWebText'] = df['OpenWebText'] / max_values['OpenWebText']
    df_normalized['Gutenberg'] = df['Gutenberg'] / max_values['Gutenberg']
    
    # Plot the normalized data
    x = np.arange(len(labels))
    width = 0.35
    
    ax.bar(x - width/2, df_normalized['OpenWebText'], width, label='OpenWebText', color='#4338ca', alpha=0.7)
    ax.bar(x + width/2, df_normalized['Gutenberg'], width, label='Gutenberg', color='#0ea5e9', alpha=0.7)
    
    ax.set_ylabel('Normalized Value')
    ax.set_title(title)
    ax.set_xticks(x)
    ax.set_xticklabels(labels)
    plt.xticks(rotation=45)
    ax.legend()
    
    fig.tight_layout()
    
    # Add annotations with actual values
    for i, v in enumerate(df['OpenWebText']):
        if i == 3:  # Format lexical diversity differently
            ax.text(i - width/2, df_normalized['OpenWebText'][i] + 0.05, f"{v:.2f}", 
                    ha='center', va='bottom', fontsize=9)
        else:
            ax.text(i - width/2, df_normalized['OpenWebText'][i] + 0.05, f"{v:.1f}", 
                    ha='center', va='bottom', fontsize=9)
            
    for i, v in enumerate(df['Gutenberg']):
        if i == 3:  # Format lexical diversity differently
            ax.text(i + width/2, df_normalized['Gutenberg'][i] + 0.05, f"{v:.2f}", 
                    ha='center', va='bottom', fontsize=9)
        else:
            ax.text(i + width/2, df_normalized['Gutenberg'][i] + 0.05, f"{v:.1f}", 
                    ha='center', va='bottom', fontsize=9)
    
    # Convert plot to image
    buf = io.BytesIO()
    plt.savefig(buf, format='png', dpi=100)
    buf.seek(0)
    plt.close(fig)
    
    return buf

# Define the app
def main():
    # Header
    st.title("üìù Generative AI: Text Models Comparison")
    st.markdown("""
    This interactive app demonstrates the differences between two text generation models:
    - **OpenWebText Model**: Trained on modern web content
    - **Gutenberg Model**: Trained on classic literature
    
    Try different prompts and settings to see how each model generates text with its unique style!
    """)
    
    st.markdown("---")
    
    # Sidebar for controls
    st.sidebar.header("Model Settings")
    
    # Sample prompts dropdown
    sample_prompts = [
        "Select a sample prompt...",
        "The future of artificial intelligence is",
        "Once upon a time in a distant land",
        "The meaning of life is",
        "To be or not to be",
        "In the beginning"
    ]
    
    selected_prompt = st.sidebar.selectbox("Sample Prompts", sample_prompts)
    
    # Text input for custom prompt
    if selected_prompt == "Select a sample prompt...":
        prompt = st.sidebar.text_area("Enter your prompt:", 
                                      "The future of artificial intelligence is", 
                                      height=100)
    else:
        prompt = selected_prompt
        
    # Other parameters
    max_length = st.sidebar.slider("Maximum Length (words)", 50, 300, 150)
    temperature = st.sidebar.slider("Temperature (creativity)", 0.1, 1.5, 0.8, 
                                  help="Higher values make output more random/creative")
    
    # About section in sidebar
    st.sidebar.markdown("---")
    st.sidebar.header("About")
    st.sidebar.info("""
    This Streamlit app demonstrates the output of two GPT-2 models fine-tuned on different datasets. 
    The full implementation with model training can be found in the Jupyter notebook.
    
    **Note:** For demonstration purposes, this app uses simulated outputs rather than actual model inference.
    """)
    
    # Generate button
    generate_pressed = st.sidebar.button("Generate Text", type="primary")
    
    # Main content area with two columns for model outputs
    col1, col2 = st.columns(2)
    
    with col1:
        st.header("OpenWebText Model")
        st.markdown("*Modern web content style*")
    
    with col2:
        st.header("Gutenberg Model")
        st.markdown("*Classic literature style*")
    
    # Process after button press
    if generate_pressed or 'openwebtext_output' in st.session_state:
        # Initialize the state if not already present
        if 'openwebtext_output' not in st.session_state:
            with st.spinner("Generating text from OpenWebText model..."):
                # Display progress bar for effect
                progress_bar = st.progress(0)
                for i in range(100):
                    time.sleep(0.01)
                    progress_bar.progress(i + 1)
                
                # Generate text
                st.session_state.openwebtext_output = simulate_openwebtext_generation(prompt, max_length, temperature)
                st.session_state.openwebtext_metrics = analyze_text(st.session_state.openwebtext_output)
                
            with st.spinner("Generating text from Gutenberg model..."):
                # Display progress bar for effect
                progress_bar = st.progress(0)
                for i in range(100):
                    time.sleep(0.01)
                    progress_bar.progress(i + 1)
                
                # Generate text
                st.session_state.gutenberg_output = simulate_gutenberg_generation(prompt, max_length, temperature)
                st.session_state.gutenberg_metrics = analyze_text(st.session_state.gutenberg_output)
                
                # Generate wordclouds and comparison plots
                st.session_state.openwebtext_wordcloud = generate_wordcloud(st.session_state.openwebtext_output)
                st.session_state.gutenberg_wordcloud = generate_wordcloud(st.session_state.gutenberg_output)
                st.session_state.metrics_plot = plot_metrics_comparison(
                    st.session_state.openwebtext_metrics, 
                    st.session_state.gutenberg_metrics,
                    title=f"Text Characteristics for Prompt: '{prompt[:30]}...'" if len(prompt) > 30 else f"Text Characteristics for Prompt: '{prompt}'"
                )
        
        # Display the generated texts
        with col1:
            st.markdown("<div class='model-output'>", unsafe_allow_html=True)
            st.write(st.session_state.openwebtext_output)
            st.markdown("</div>", unsafe_allow_html=True)
            
            # Display metrics
            st.markdown("<div class='metrics-card'>", unsafe_allow_html=True)
            st.subheader("Text Metrics")
            metrics = st.session_state.openwebtext_metrics
            col1a, col1b = st.columns(2)
            with col1a:
                st.metric("Word Count", f"{metrics['word_count']}")
                st.metric("Unique Words", f"{metrics['unique_words']}")
                st.metric("Sentences", f"{metrics['sentence_count']}")
            with col1b:
                st.metric("Avg Word Length", f"{metrics['avg_word_length']:.2f}")
                st.metric("Lexical Diversity", f"{metrics['lexical_diversity']:.2f}")
                st.metric("Avg Sentence Length", f"{metrics['avg_sentence_length']:.2f}")
            st.markdown("</div>", unsafe_allow_html=True)
            
            # Display wordcloud
            st.subheader("Word Cloud")
            st.image(st.session_state.openwebtext_wordcloud)
            
        with col2:
            st.markdown("<div class='model-output'>", unsafe_allow_html=True)
            st.write(st.session_state.gutenberg_output)
            st.markdown("</div>", unsafe_allow_html=True)
            
            # Display metrics
            st.markdown("<div class='metrics-card'>", unsafe_allow_html=True)
            st.subheader("Text Metrics")
            metrics = st.session_state.gutenberg_metrics
            col2a, col2b = st.columns(2)
            with col2a:
                st.metric("Word Count", f"{metrics['word_count']}")
                st.metric("Unique Words", f"{metrics['unique_words']}")
                st.metric("Sentences", f"{metrics['sentence_count']}")
            with col2b:
                st.metric("Avg Word Length", f"{metrics['avg_word_length']:.2f}")
                st.metric("Lexical Diversity", f"{metrics['lexical_diversity']:.2f}")
                st.metric("Avg Sentence Length", f"{metrics['avg_sentence_length']:.2f}")
            st.markdown("</div>", unsafe_allow_html=True)
            
            # Display wordcloud
            st.subheader("Word Cloud")
            st.image(st.session_state.gutenberg_wordcloud)
        
        # Display comparison chart
        st.markdown("---")
        st.header("Comparative Analysis")
        st.image(st.session_state.metrics_plot, use_column_width=True)
        
        # Add explanations about the differences
        st.markdown("""
        ### Key Observations
        
        The comparison above highlights several key differences between the two models:
        
        1. **Style and Tone**: 
           - OpenWebText model produces more contemporary, factual-sounding text
           - Gutenberg model generates more formal, literary text with archaic language
        
        2. **Vocabulary and Structure**:
           - The Gutenberg model typically uses more unique words and has higher lexical diversity
           - The Gutenberg model tends to generate longer, more complex sentences
           - The OpenWebText model uses more modern terminology and references
        
        3. **Content Patterns**:
           - OpenWebText often includes references to data, statistics, and recent research
           - Gutenberg text features more descriptive language and metaphorical expressions
        
        These differences reflect the distinct nature of the training datasets: modern web content versus classic literature.
        """)
        
        # Reset button
        if st.button("Reset and Try Again"):
            # Clear session state
            for key in ['openwebtext_output', 'gutenberg_output', 'openwebtext_metrics', 
                       'gutenberg_metrics', 'openwebtext_wordcloud', 'gutenberg_wordcloud',
                       'metrics_plot']:
                if key in st.session_state:
                    del st.session_state[key]
            st.experimental_rerun()

# Run the app
if __name__ == "__main__":
    main()
